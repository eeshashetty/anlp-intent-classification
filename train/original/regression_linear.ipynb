{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor, Ridge)\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from math import sqrt\n",
    "\n",
    "from scipy.stats import mode, kendalltau, pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-698820a3d496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenize,mwt,pos,lemma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "parser = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def tokenize_text(text, lemmatize=False):\n",
    "    doc = parser(text)\n",
    "    if lemmatize:\n",
    "        return [word.text for sent in doc.sentences for word in sent.words]\n",
    "    else:\n",
    "        return [word.lemma for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    loss = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(y_pred)\n",
    "    print('rmse: ', loss)\n",
    "    return loss\n",
    "\n",
    "rmse_score = make_scorer(rmse_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(splits_dir, train, dev, is_logistic=False, id_column=0, target_column=26, text_column=2):\n",
    "    train_x, train_y = get_split(splits_dir, train, is_logistic, id_column, target_column, text_column)\n",
    "    dev_x, dev_y = get_split(splits_dir, dev, is_logistic, id_column, target_column, text_column)\n",
    "    return train_x, train_y, dev_x, dev_y\n",
    "\n",
    "def get_split(splits_dir, split_file, is_logistic, id_column, target_column, text_column):\n",
    "    split_file_path = os.path.join(splits_dir, split_file)\n",
    "    with open(split_file_path) as f:\n",
    "        split_reader = csv.reader(f, delimiter='\\t')\n",
    "        split_data = list(split_reader)[1:]  # skip header\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for line in split_data:\n",
    "        if is_logistic:\n",
    "            y.append(int(float((line[target_column]))*10))\n",
    "        else:\n",
    "            y.append(float(line[target_column]))\n",
    "        x.append(line[text_column])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_cv(train_x, train_y, dev_x, dev_y):\n",
    "    x = np.concatenate([train_x, dev_x])\n",
    "    y = np.concatenate([train_y, dev_y])\n",
    "\n",
    "    # create cv iterator object\n",
    "    test_fold = np.concatenate([\n",
    "                                    # The training data\n",
    "                                    np.ones(train_x.shape[0], dtype=np.int8)*-1,\n",
    "                                    # The development data\n",
    "                                    np.zeros(dev_x.shape[0], dtype=np.int8)])\n",
    "    cv_train_dev = PredefinedSplit(test_fold)\n",
    "\n",
    "    return x, y, cv_train_dev\n",
    "\n",
    "def run_grid_search(x, y, pipeline, parameters, cv_iter):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=cv_iter, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "    grid_search.fit(x, y)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters\n",
    "\n",
    "def search_linear(train_x, train_y, dev_x, dev_y):\n",
    "    best_params = []\n",
    "    # perform grid search over train and dev\n",
    "    x, y, cv_train_dev = create_train_dev_cv(train_x, train_y, dev_x, dev_y)\n",
    "    \n",
    "    estimators = [#('OLS', LinearRegression()),\n",
    "                  ('Ridge', Ridge(random_state=RANDOM_SEED)),\n",
    "                  #('KernelRidge', KernelRidge(alpha=1, kernel='linear', gamma=None, degree=3, coef0=1))\n",
    "              #('Theil-Sen', TheilSenRegressor(random_state=42)), gets error: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
    "              #('RANSAC', RANSACRegressor(min_samples=len(dev_x), random_state=42)),\n",
    "              #('HuberRegressor', HuberRegressor())\n",
    "                ]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        estimator,\n",
    "        ])\n",
    "        # uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "            #'vect__max_df': (0.75, 1.0),\n",
    "            #'vect__min_df': (0.75, 1.0),\n",
    "            'vect__ngram_range': ((1, 1), (1, 2), (1,3)),  # unigrams, bigrams, or trigrams\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            'tfidf__norm': ('l1', 'l2'),\n",
    "            'Ridge__alpha': (0.5, 1.0, 1.5),\n",
    "            'Ridge__tol': (0.0001, 0.01),\n",
    "            #'clf__estimator__kernel': ('linear','poly', 'rbf', 'sigmoid'),\n",
    "        }\n",
    "        print('Running grid search for estimator: ', estimator)\n",
    "        best_params.append(run_grid_search(x, y, pipeline, parameters, cv_train_dev))\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits_dir = '../../data/gold_cv_dev_data/CongressionalHearingFolds/fold0'\n",
    "train='train.tsv'\n",
    "dev='test.tsv'\n",
    "\n",
    "train_x, train_y, dev_x, dev_y = get_splits(splits_dir, train, dev)\n",
    "# best_params = search_linear(train_x, train_y, dev_x, dev_y)\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.579380164285695,\n",
       " 'For my definition of a small business, I rely on the Small Business Administration. They define it for us and the entire Federal Government. And it varies industry to industry.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0],train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge RMSE:  0.30574382199995925\n",
      "Ridge Kendall:  0.2030462612784842\n",
      "Ridge Pearson:  0.3683692257369324\n",
      "Ridge Spearman:  0.28202613821840333\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True, \n",
    "                                     ngram_range=(1, 3))),\n",
    "            ('tfidf', TfidfTransformer(norm='l2')),\n",
    "            ('Ridge', Ridge(alpha=1.5, tol=0.0001, random_state=RANDOM_SEED)),\n",
    "            ])\n",
    "pipeline.fit(train_x, train_y)\n",
    "preds = pipeline.predict(dev_x)\n",
    "rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "print('Ridge RMSE: ', rmse)\n",
    "print('Ridge Kendall: ', kendalltau(dev_y, preds)[0])\n",
    "print('Ridge Pearson: ', pearsonr(dev_y, preds)[0])\n",
    "print('Ridge Spearman: ', spearmanr(dev_y, preds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Freq entropy: 0.0\n",
      "RMSE using most freq entropy: 0.5102494215256714\n",
      "Mean entropy:  0.33980704680727497\n",
      "RMSE using mean entropy: 0.33243672623925663\n"
     ]
    }
   ],
   "source": [
    "entropy_most_freq = mode(train_y)[0][0]\n",
    "entropy_mean = np.mean(train_y)\n",
    "\n",
    "rmse_most_freq = np.sqrt(mean_squared_error(dev_y, [entropy_most_freq]*len(dev_y)))\n",
    "rmse_mean_ent = np.sqrt(mean_squared_error(dev_y, [entropy_mean]*len(dev_y)))\n",
    "print('Most Freq entropy:', entropy_most_freq)\n",
    "print('RMSE using most freq entropy:', rmse_most_freq)\n",
    "print('Mean entropy: ', entropy_mean)\n",
    "print('RMSE using mean entropy:', rmse_mean_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598195083636408"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_y[np.where(train_y !=0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-648366dc89a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmord\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mord'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, FunctionTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import mord as m\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_estimators_logistic(train_x, train_y, dev_x, dev_y, \n",
    "                   lad_epsilon=0.0, lad_tol=0.0001, lad_C=1.0, lad_loss='l1',\n",
    "                   it_alpha=1.0,\n",
    "                   at_alpha=1.0):\n",
    "    model_to_preds = {}\n",
    "    estimators = [('LAD', m.LAD(epsilon=lad_epsilon, tol=lad_tol, C=lad_C, loss=lad_loss, fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, max_iter=1000, random_state=RANDOM_SEED)),\n",
    "                  #('LogisticIT', m.LogisticIT(alpha=it_alpha, verbose=0)),\n",
    "                  #('LogisticAT', m.LogisticAT(alpha=at_alpha, verbose=0))\n",
    "                  #Values in y must be [0 1 2 3 4]\n",
    "                 ]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            estimator,\n",
    "            ])\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        preds = pipeline.predict(dev_x)\n",
    "        rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "        model_to_preds[pipeline] = preds\n",
    "        print('Estimator: ', estimator, ' RMSE: ', rmse)\n",
    "    return model_to_preds\n",
    "\n",
    "def run_estimators(train_x, train_y, dev_x, dev_y, \n",
    "                   ridge_alpha=1.0, ridge_tol=0.001):\n",
    "    model_to_preds = {}\n",
    "    estimators = [('OrdinalRidge', m.OrdinalRidge(alpha=ridge_alpha, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=ridge_tol, solver='auto', random_state=RANDOM_SEED))]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            estimator,\n",
    "            ])\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        preds = pipeline.predict(dev_x)\n",
    "        rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "        kend = kendalltau(dev_y, preds)[0]\n",
    "        pear = pearsonr(dev_y, preds)[0]\n",
    "        spear = spearmanr(dev_y, preds)[0]\n",
    "        model_to_preds[pipeline] = preds\n",
    "        print('Estimator: ', estimator, '\\n  RMSE:', rmse, '\\tKendall:', kend, '\\tPearson:', pear, '\\tSpearman: ', spear)\n",
    "    return model_to_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits_dir = '../../data/splits_folds_ordered_response_06-19/fold0'\n",
    "train='train.tsv'\n",
    "dev='test.tsv'\n",
    "\n",
    "train_x, train_y, dev_x, dev_y = get_splits(splits_dir, train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_alphas = [0.5, 1.0, 1.5]\n",
    "ridge_tols = [0.0001, 0.001, 0.01]\n",
    "model_dicts=[]\n",
    "for alpha in ridge_alphas:\n",
    "    for tol in ridge_tols:\n",
    "        print('alpha: ', alpha, ', tol: ', tol)\n",
    "        model_dicts.append(run_estimators(train_x, train_y, dev_x, dev_y, ridge_alpha=alpha, ridge_tol=tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x_int, train_y_int, dev_x_int, dev_y_int = get_splits(splits_dir, train, dev, is_logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_dicts = run_estimators_logistic(train_x_int, train_y_int, dev_x_int, dev_y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_log = np.sqrt(mean_squared_error(dev_y_int, [0]*len(dev_y_int)))\n",
    "print(rmse_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
